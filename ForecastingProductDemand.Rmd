---
title: "Forecasting Product Demand"
author: "Ryan Hawkins"
date: "November 13, 2018"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Importing data

```{r message=FALSE, warning=FALSE}
# Load xts package 
library(xts)
library(fpp2)

# Create the dates object as an index for your xts object
dates <- seq(as.Date("2014-01-19"), length = 176, by = "weeks")

bev <- read.csv("Bev.csv")

# Create an xts object called bev_xts
bev_xts <- xts(bev, order.by = dates)
```

## Plotting / visualizing data

In the videos we are working with the mountain region of the beverage data. Here in the exercises you will be working with the metropolitan areas of the state to forecast their products.

There are three products in the metropolitan areas - high end, low end, and specialty. The specialty product is not sold any where else in the state. The column names for the sales of these three products are MET.hi, MET.lo, and MET.sp respectively. Before looking at each one of these products individually, let's plot how total sales are going in the metropolitan region. The object bev_xts has been preloaded into your workspace.

```{r}
# Create the individual region sales as their own objects
MET_hi <- bev_xts[,"MET.hi"]
MET_lo <- bev_xts[,"MET.lo"]
MET_sp <- bev_xts[,"MET.sp"]

# Sum the region sales together
MET_t <- MET_hi + MET_lo + MET_sp

# Plot the metropolitan region total sales
plot(MET_t)
```

## auto.arima() function

We can use the auto.arima function to help us automatically select a good starting model to build. Your regional sales data summed up for all products in the metropolitan region is loaded in your workspace as the MET_t object. We are going to use the index function to help with these dates.

```{r}
# Split the data into training and validation
MET_t_train <- MET_t[index(MET_t) < "2017-01-01"]
MET_t_valid <- MET_t[index(MET_t) >= "2017-01-01"]

# Use auto.arima() function for metropolitan sales
MET_t_model <- auto.arima(MET_t_train)
```

## Forecast function

Previously you built a model and saved it as MET_t_model. Now we need to forecast out the values of this model for the first 5 months of 2017.
```{r}
# Forecast the first 22 weeks of 2017
forecast_MET_t <- forecast(MET_t_model, h = 22)

# Plot this forecast #
plot(forecast_MET_t)
```

## Calculating MAPE and MAE

You previously calculated the forecast for the metropolitan region total sales and stored it in the object forecast_MET_t. You also have your validation data set stored in the object MET_t_valid that covers the same first 22 weeks of 2017

```{r}
# Convert to numeric for ease
for_MET_t <- as.numeric(forecast_MET_t$mean)
v_MET_t <- as.numeric(MET_t_valid)

# Calculate the MAE
MAE <- mean(abs(v_MET_t - for_MET_t ))

# Calculate the MAPE
MAPE <- 100*mean(abs(v_MET_t - for_MET_t )/v_MET_t)

# Print to see how good your forecast is!
print(MAE)
print(MAPE)
```

## Visualizing Forecast

Your forecast seemed to be off by over 18% on average. Let's visually compare your forecast with the validation data set to see if we can see why. Your workspace has your forecast object forecast_MET_t and validation data set MET_t_valid loaded for you. Don't forget your validation is 22 weeks long!

```{r}
# Convert your forecast to an xts object
for_dates <- seq(as.Date("2017-01-01"), length = 22, by = "weeks")
for_MET_t_xts <- xts(forecast_MET_t$mean, order.by = for_dates)

# Plot the validation data set
plot(MET_t_valid, main = 'Forecast Comparison', ylim = c(4000, 8500))

# Overlay the forecast of 2017
lines(for_MET_t_xts, col = "blue")
```

## Confidence Intervals for Forecast

Your forecast object forecast_MET_t not only has the forecast, but also margin of error calculations for the forecast called confidence intervals. These confidence intervals show us a wiggle room on our forecasts since no forecast is ever perfect.

The forecast was stored as the object forecast_MET_t\$mean. The upper and lower limit of this interval is stored similarly. The first column of the upper confidence interval is the 80% confidence interval and can be accessed via forecast_MET_t\$upper[,1]. The second column is the 95% confidence interval. To get the lower limit replace the word 

```{r}
# Plot the validation data set
plot(MET_t_valid, main = 'Forecast Comparison', ylim = c(4000, 8500))

# Overlay the forecast of 2017
lines(for_MET_t_xts, col = "blue")

# Convert the limits to xts objects
lower <- xts(forecast_MET_t$lower[,2], order.by = for_dates)
upper <- xts(forecast_MET_t$upper[,2], order.by = for_dates)

# Adding confidence intervals of forecast to plot
lines(lower, col = "blue", lty = "dashed")
lines(upper, col = "blue", lty = "dashed")
```

## Calculating price elasticity
Now that you know about price elasticities, let's see how elastic prices are for the high end product in the metropolitan region! Grand training and validation data sets have already been created for you and are stored in the objects bev_xts_train and bev_xts_valid.

You already have the sales for the high end product loaded in the workspace as MET_hi. You first need to extract the prices out of the bev_xts_train object. The column names for prices in the bev_xts_train object is MET.hi.p.

```{r}

bev_xts_train <- bev_xts[index(bev_xts) < "2017-01-01"]
bev_xts_valid <- bev_xts[index(bev_xts) >= "2017-01-01"]
# Save the prices of each product
l_MET_hi_p <- log(as.vector(bev_xts_train[,'MET.hi.p']))
MET_hi <- as.vector(bev_xts_train[,'MET.hi'])

# Save as a data frame
MET_hi_train <- data.frame(as.vector(log(MET_hi)), l_MET_hi_p)
colnames(MET_hi_train) <- c("log_sales", "log_price")

# Calculate the regression
model_MET_hi <- lm(log_sales ~ log_price, data = MET_hi_train)
```

## Create holiday / promotional effect variables

We saw some notion of seasonality in the previous exercise, but let's test to make sure that something actually is there. Your bosses think that their products would be more desired around the weeks of Christmas, New Year's, and Valentine's Day. The marketing department also mentions that they have been running promotional deals the week before Mother's Day the previous 5 years. Let's create a binary indicator variable for New Year's!

```{r}
# Create date indices for New Year's week
n.dates <- as.Date(c("2014-12-28", "2015-12-27", "2016-12-25"))

# Create xts objects for New Year's
newyear <- as.xts(rep(1, 3), order.by = n.dates)

# Create sequence of dates for merging
dates_train <- seq(as.Date("2014-01-19"), length = 154, by = "weeks")

# Merge training dates into New Year's object
newyear <- merge(newyear, dates_train, fill = 0)
```

## Regression for holiday / promotional effects

Now that you have created the New Year's indicator variable, let's see if it is significantly different than the usual sales pattern using regression.

Your data.frame with log of sales and log of prices is saved in your workspace as MET_hi_train. Your New Year's variable is stored as newyear.

```{r}
# Create MET_hi_train_2 by adding newyear
MET_hi_train_2 <- data.frame(MET_hi_train, as.vector(newyear))
colnames(MET_hi_train_2)[3] <- "newyear"

# Build regressions for the product
model_MET_hi_full <- lm(log_sales ~ log_price + newyear, data = MET_hi_train_2)

summary(model_MET_hi_full)
```


# Forecasting with Regression

## Create future predictor variables

To get forecasts for the regression models with price, holidays, and promotions we need future values for each of these inputs! For our high end product we previously determined holidays and promotions aren't important to the model. Prices are negotiated 6 months in advance, so the prices in the validation data set are reasonable to use for forecast calculations. Your validation data set is stored in your workspace as bev_xts_valid.

```{r}
# Subset the validation prices #
l_MET_hi_p_valid <- as.vector(log(bev_xts_valid[,"MET.hi.p"]))

# Create a validation data frame #
MET_hi_valid <- data.frame(l_MET_hi_p_valid)
colnames(MET_hi_valid) <- "log_price"

```

## Forecast future values of demand

Perfect! You have created the estimated future prices. Now you just need to score these with your regression models to forecast future sales demand for your high end product.

Your validation predictions are in your workspace as MET_hi_valid. The model model_MET_hi_full included all possible variables, but prices were the only thing important in the model, so your model with only prices is saved in your workspace as model_MET_hi.

```{r}
# Predict the log of sales for your high end product
pred_MET_hi <- predict(model_MET_hi, MET_hi_valid)

# Convert predictions out of log scale
pred_MET_hi <- exp(pred_MET_hi)

```

## Visualizing forecasts of regression

Wonderfully done! You now have a forecast for the high end product in the metropolitan region. Let's visualize this forecast!.

```{r}
# Convert to an xts object
dates_valid <- seq(as.Date("2017-01-01"), length = 22, by = "weeks")
pred_MET_hi_xts <- xts(pred_MET_hi, order.by = dates_valid)

# Plot the forecast
plot(pred_MET_hi_xts)

# Calculate and print the MAPE
MET_hi_v <- bev_xts_valid[,"MET.hi"]

MAPE <- 100*mean(abs((pred_MET_hi_xts - MET_hi_v)/MET_hi_v))
print(MAPE)
```

## Calculating residuals from regression

Your previous model that predicts the log of sales from the log of price along with holiday and promotion effects is saved in the model_MET_hi_full object. Your data frame that has the data that built this model is in the object MET_hi_train. Now let's see where we went wrong.

```{r}
# Calculate the residuals from the model
MET_hi_full_res <- residuals(model_MET_hi_full)

# Convert the residuals to an xts object
MET_hi_full_res <- xts(MET_hi_full_res, order.by = dates_train)

# Plot the histogram of the residuals
hist(MET_hi_full_res)

# Plot the residuals over time
plot(MET_hi_full_res)
```

## Forecast Residuals

```{r}
# Build an ARIMA model on the residuals: MET_hi_arima
MET_hi_arima <- auto.arima(MET_hi_full_res)

# Look at a summary of the model
summary(MET_hi_arima)

# Forecast 22 weeks with your model: for_MET_hi_arima
for_MET_hi_arima <- forecast(MET_hi_arima, h= 22)

# Print first 10 observations
head(for_MET_hi_arima, n = 10)

# Convert your forecasts into an xts object
dates_valid <- seq(as.Date("2017-01-01"), length = 22, by = "weeks")
for_MET_hi_arima <- xts(for_MET_hi_arima$mean, order.by = dates_valid)

# Plot the forecast
plot(for_MET_hi_arima)
```

## Combining residuals from regression & time series

The residuals from the two different models that you have developed now need to be combined together! Once they are combined we will have our full forecast. 

```{r}
# Convert your residual forecast to the exponential version
for_MET_hi_arima <- exp(for_MET_hi_arima)

# Multiply your forecasts together!
for_MET_hi_final <- for_MET_hi_arima * pred_MET_hi_xts
```

## Visualizing transfer function forecast

It would be great to compare this forecast to the validation data set that we held out from the beginning. First, let's visualize these two forecasts against each other. 

```{r}
# Plot the final forecast - don't touch the options!
plot(for_MET_hi_final, ylim = c(1000, 4300))

# Overlay the validation data set
lines(MET_hi_v, col = "blue")

# Calculate the MAE
MAE <- mean(abs(for_MET_hi_final - MET_hi_v))
print(MAE)

# Calculate the MAPE
MAPE <- 100*mean(abs((for_MET_hi_final - MET_hi_v)/MET_hi_v))
print(MAPE)
```

## ARIMA Forecasting

Before we build an ensemble model we need an actual time series forecast of the demand itself so we can average the forecasts. The training data object MET_hi is saved in your workspace as well as the validation data MET_hi_v.

```{r}
# Build an ARIMA model using the auto.arima function
MET_hi_model_arima <- auto.arima(MET_hi)

# Forecast the ARIMA model
for_MET_hi <- forecast(MET_hi_model_arima, h = 22)

# Save the forecast as an xts object
dates_valid <- seq(as.Date("2017-01-01"), length = 22, by = "weeks")
for_MET_hi_xts <- xts(for_MET_hi$mean, order.by = dates_valid)

# Calculate the MAPE of the forecast
MAPE <- 100 * mean(abs((for_MET_hi_xts - MET_hi_v)/MET_hi_v))
print(MAPE)
```

## Ensembling of Forecasts

Now that you have two forecasts (your regression and your time series) we can ensemble their forecasts to see if that helps the forecasts. Your two forecasts are saved as for_MET_hi_xts and pred_MET_hi_xts in your workspace as well as the validation data set MET_hi_v
```{r}
# Ensemble the two forecasts together
for_MET_hi_en <- 0.5*(for_MET_hi_xts + pred_MET_hi_xts)

# Calculate the MAE and MAPE
MAE <- mean(abs(for_MET_hi_en - MET_hi_v))
print(MAE)

MAPE <- 100 * mean(abs((for_MET_hi_en - MET_hi_v)/MET_hi_v))
print(MAPE)
```

# Hierarchical forecasting

## Build time series forecast for new product

Before we can even calculate a bottom-up forecast for the metropolitan region we need to have forecasts of multiple products! First, let's build a time series forecast of the specialty product in the metropolitan region. The product demand is saved as MET_sp in your workspace as well as dates_valid as well as your validation data MET_sp_v.

You've written the MAPE function enough at this point. A mape() function has now been written for you to use with two inputs: the first is the forecast and the second is the validation set.

```{r}
library(Metrics)

MET_sp <- bev_xts[,"MET.sp"]

MET_sp <- MET_t[index(MET_t) < "2017-01-01"]
MET_sp_v <- MET_t[index(MET_t) >= "2017-01-01"]

# Build a time series model 
MET_sp_model_arima <- auto.arima(MET_sp)

# Forecast the time series model for 22 periods
for_MET_sp <- forecast(MET_sp_model_arima, h = 22)

# Create an xts object
for_MET_sp_xts <- xts(for_MET_sp$mean, order.by = dates_valid)

# Calculate the MAPE
MAPE <- mape(for_MET_sp_xts, MET_sp_v)
print(MAPE)
```

## Build regression forecast for new product

We saw in a previous exercise that regression forecasts are also worth building! Your workspace has some preloaded things to help. You have a data frame called MET_sp_train with the variables log_sales, log_price, christmas, valentine, newyear, and mother in it. Your workspace also has a validation data frame MET_sp_valid for predictions.

Currently not working I need to figure out how to build the train dataframe with logsales, logprice, newyear, christmas, valentine, and mother then a validate with -logsales

```{r eval=FALSE, include=FALSE}
# Save the prices of each product
l_MET_sp_p <- log(as.vector(bev_xts_train[,'MET.sp.p']))
MET_sp <- as.vector(bev_xts_train[,'MET.sp'])

# Save as a data frame
MET_sp_train <- data.frame(as.vector(log(MET_sp)), l_MET_sp_p)
colnames(MET_sp_train) <- c("log_sales", "log_price")

# Calculate the regression
model_MET_sp <- lm(log_sales ~ log_price, data = MET_sp_train)

# Build a regression model
model_MET_sp_full <- lm(log_sales~., data = MET_sp_train)

# Forecast the regression model
pred_MET_sp <- predict(model_MET_sp_full, MET_sp_valid)

# Exponentiate your predictions and create an xts object
pred_MET_sp <- exp(pred_MET_sp)
pred_MET_sp_xts <- xts(pred_MET_sp, order.by = dates_valid)

# Calculate MAPE
MAPE <- mape(pred_MET_sp_xts, MET_sp_v)
print(MAPE)

# Ensemble the two forecasts
for_MET_sp_en <- 0.5*(for_MET_sp_xts + pred_MET_sp_xts)

# Calculate the MAPE
MAPE <- mape(for_MET_sp_en, MET_sp_v)
print(MAPE)

# Calculate the metropolitan regional sales forecast
for_MET_total <- pred_MET_hi_xts + for_MET_sp_en + pred_MET_lo_xts 

# Calculate a validation data set 
MET_t_v <- bev_xts_valid[,"MET.hi"] + bev_xts_valid[,"MET.lo"] + bev_xts_valid[,"MET.sp"]

# Calculate the MAPE
MAPE <- mape(for_MET_total, MET_t_v)
print(MAPE)
```

## Build time series forecast at regional level

Sometimes you don't have the time to build forecasts for every product, so we take a top-down approach to hierarchical forecasting. Let's work on the metropolitan region in reverse this time!

Your workspace has the following objects pre-loaded: MET_total for the total regional sales, dates_valid, and MET_t_v for the valiation data set.

```{r eval=FALSE, include=FALSE}
# Build a regional time series model
MET_t_model_arima <- auto.arima(MET_total)

# Calculate a 2017 forecast for 22 periods
for_MET_t <- forecast(MET_t_model_arima, h=22)

# Make an xts object from your forecast
for_MET_t_xts <- xts(for_MET_t$mean, order.by=dates_valid)

# Calculate the MAPE
MAPE <- mape(for_MET_t_xts, MET_t_v)
print(MAPE)

# Calculate the average historical proportions
prop_hi <- mean(MET_hi/MET_total)
prop_lo <- mean(MET_lo/MET_total)
prop_sp <- mean(MET_sp/MET_total)

# Distribute out your forecast to each product
for_prop_hi <- prop_hi*for_MET_t_xts
for_prop_lo <- prop_lo*for_MET_t_xts
for_prop_sp <- prop_sp*for_MET_t_xts

# Calculate the MAPE's for each product
MAPE_hi <- mape(for_prop_hi, MET_hi_v)
print(MAPE_hi)

MAPE_lo <- mape(for_prop_lo, MET_lo_v)
print(MAPE_lo)

MAPE_sp <- mape(for_prop_sp, MET_sp_v)
print(MAPE_sp)

# Calculate the average historical proportions
prop_hi_2 <- mean(MET_hi)/mean(MET_total)
prop_lo_2 <- mean(MET_lo)/mean(MET_total)
prop_sp_2 <- mean(MET_sp)/mean(MET_total)
# Distribute out your forecast to each product
for_prop_hi_2 <- prop_hi_2*for_MET_t_xts
for_prop_lo_2 <- prop_lo_2*for_MET_t_xts
for_prop_sp_2 <- prop_sp_2*for_MET_t_xts
# Calculate the MAPEs for each product
MAPE_hi <- mape(for_prop_hi_2, MET_hi_v)
print(MAPE_hi)
MAPE_lo <- mape(for_prop_lo_2, MET_lo_v)
print(MAPE_lo)
MAPE_sp <- mape(for_prop_sp_2, MET_sp_v)
print(MAPE_sp)
```

